{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "<p>Today, implementations for neural networks are widely available on PyTorch, TensorFlow, etc. While this is convenient for the average user, for the learner this leaves a lot of knowledge in the realm of theory and abstraction. In this project I build a CNN using numpy and train it on the <strong>Fashion MNIST</strong> dataset to perform a simple classification task. The purpose of this exercise is to familiarize myself with the details of neural networks by building the architecture for a Convoluted Neural Network (CNN) and training it on an image recognition task. I'm much more familiar with feed-forward neural networks (FNN's) since I mostly work with language data, so this will be an additional challenge in learning a new architecture.</p>\n",
    "\n",
    "##### Tutorials\n",
    "- https://www.geeksforgeeks.org/introduction-convolution-neural-network/\n",
    "- https://towardsdatascience.com/a-guide-to-convolutional-neural-networks-from-scratch-f1e3bfc3e2de\n",
    "- https://victorzhou.com/blog/intro-to-cnns-part-1/\n",
    "- https://victorzhou.com/blog/intro-to-cnns-part-2/\n",
    "- https://towardsdatascience.com/convolution-vs-cross-correlation-81ec4a0ec253\n",
    "- https://www.youtube.com/watch?v=Lakz2MoHy6o\n",
    "##### Data\n",
    "- https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html#torchvision.datasets.FashionMNIST\n",
    "- https://github.com/zalandoresearch/fashion-mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convoluted Neural Networks\n",
    "<p>Neural networks can generally be understood through three layers: the input, hidden, and output layers. The input layer takes some structured data and transforms them (based on learned weights) for the hidden layer. The hidden layer consecutively applies weights to the data as well as a non-linear activation function before passing to the output layer. The output layer uses weights and an activation function to return the number of outputs required by the task. For image classification, this will be a vector whose values correspond to the probability that the input image belongs to each class (the softmax of the vector is therefore the classification we're looking for).</p>\n",
    "<p>FNN's have been largely replaced in both NLP and computer vision by Recursive Neural Networks (RNN's) and CNN's respectively, due to their inability to accurately maintain the structure of the data. The input layer of FNN's reduce the image to a one dimensional vector which loses the spatial information encoded in the image. A square 32 pixel RGB image is best represented as a 32x32x3 array but will be transformed into a 3072x1 array by the FNN's input layer. CNN's preserve the structure of images using convolution, which entails running a number of smaller neural networks (which takes a smaller input such as 5x5x3) across the whole image. Each network, called a <strong>kernel</strong>, will output a 2D image representing some aspect that we are interested in learned using training data. These outputs are stacked on top of each other and run through a number of layers before being flattened again into a 2D array for the output layer, which is generally an FNN.</p>\n",
    "<p>In the scope of this project I implement the convolutional mechanism </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.26.3)\n",
      "Requirement already satisfied: torchvision in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.16.2)\n",
      "Requirement already satisfied: torch in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: tqdm in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (4.66.1)\n",
      "Requirement already satisfied: matplotlib in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (3.8.2)\n",
      "Requirement already satisfied: requests in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 2)) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from torchvision->-r requirements.txt (line 2)) (10.2.0)\n",
      "Requirement already satisfied: filelock in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (4.9.0)\n",
      "Requirement already satisfied: sympy in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from torch->-r requirements.txt (line 3)) (2023.12.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (4.47.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from matplotlib->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 5)) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from requests->torchvision->-r requirements.txt (line 2)) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/alvinchen/anaconda3/envs/cnn/lib/python3.10/site-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "\n",
    "traintensors = datasets.FashionMNIST(root=\"./data\", train=True, download=True)\n",
    "testtensors = datasets.FashionMNIST(root=\"./data\", train=False, download=True)\n",
    "trainset = np.array(traintensors.data)\n",
    "trainlabels = np.array(traintensors.targets)\n",
    "testset = np.array(testtensors.data)\n",
    "testlabels = np.array(testtensors.targets)\n",
    "classes = (\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, kernel_size=5, padding=0):\n",
    "    if len(data.shape) == 3:\n",
    "        data = data[:, np.newaxis, :, :]\n",
    "    elif len(data.shape) == 4 and data.shape[3] == 3:\n",
    "        data = np.moveaxis(data, -1, 1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Layer\n",
    "The core of CNN's relies on convlution: a 5x5 <strong>kernel</strong> (other odd-numbered square matrices are also possible) that is multiplied elementwise on the image and summed up. The kernel is iterated across the image in <strong>strides</strong> and are positionally combined to form a 2D array. However, since kernels are larger than one pixel, the resulting array will be smaller than the original image. In order to produce resulting arrays of the correct size, we <strong>pad</strong> the array with borders of zeroes. For a 5x5 kernel, the border must be size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, input_shape, kernel_size=5, num_kernels=6, padding=0):\n",
    "        # Get input dimensions\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.d = input_depth\n",
    "        self.h = input_height + kernel_size - 1\n",
    "        self.w = input_width + kernel_size - 1\n",
    "        self.input_shape = input_shape\n",
    "        # Initialize kernels and bias\n",
    "        self.padding = padding\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pad_size = kernel_size // 2\n",
    "        self.kernel_shape = (self.num_kernels, self.d, self.kernel_size, self.kernel_size)\n",
    "        self.bias_shape = (self.num_kernels, self.h - self.kernel_size + 1, self.w - self.kernel_size + 1)\n",
    "        # Dividing mimics Xavier Initialization and reduces variance\n",
    "        self.kernels = np.random.randn(*self.kernel_shape) / (self.kernel_size * self.kernel_size)\n",
    "        self.bias = np.random.randn(*self.bias_shape) / (self.h * self.w)\n",
    "    \n",
    "    def iter_regions(self, image):\n",
    "        for i in range(self.h - self.kernel_size + 1):\n",
    "            for j in range(self.w - self.kernel_size + 1):\n",
    "                im_region = image[:, i:(i + self.kernel_size), j:(j + self.kernel_size)]\n",
    "                yield im_region, i, j\n",
    "    \n",
    "    def forward(self, input):\n",
    "        padded = np.pad(input, ((0,0), (self.pad_size, self.pad_size), (self.pad_size, self.pad_size)), mode=\"constant\", constant_values=self.padding)\n",
    "        self.prev_input = padded # Save for backpropagation, padding needed\n",
    "        self.output = np.copy(self.bias)\n",
    "        for im_region, i, j in self.iter_regions(padded):\n",
    "            self.output[:, i, j] += np.sum(im_region * self.kernels, axis=(1, 2, 3))\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        # Cross correlation for kernel gradient\n",
    "        d_L_d_kernels = np.zeros(self.kernels.shape)\n",
    "        for im_region, i, j in self.iter_regions(self.prev_input):\n",
    "            for f in range(self.num_kernels):\n",
    "                d_L_d_kernels[f] += d_L_d_out[f, i, j] * im_region \n",
    "        # Full convolution for input gradient\n",
    "        d_L_d_input = np.zeros(self.input_shape)\n",
    "        padded_d_L_d_out = np.pad(d_L_d_out, ((0,0), (self.pad_size, self.pad_size), (self.pad_size, self.pad_size)), mode=\"constant\", constant_values=0)\n",
    "        conv_kernels = np.rot90(np.moveaxis(self.kernels, 0, 1), 2, axes=(2, 3))\n",
    "        for im_region2, i, j in self.iter_regions(padded_d_L_d_out):\n",
    "            for d in range(self.d):\n",
    "                d_L_d_input[d, i, j] += np.sum(im_region2 * conv_kernels[d])\n",
    "        # Adjust by learn rate\n",
    "        self.bias -= learn_rate * d_L_d_out\n",
    "        self.kernels -= learn_rate * d_L_d_kernels\n",
    "        return d_L_d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.prev_input = input\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        d_L_d_out[self.prev_input < 0] = 0\n",
    "        return d_L_d_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling Layer\n",
    "Since convolutions capture information from neighboring pixels, many elements in the output array are redundant. <strong>Pooling</strong> presents a simple solution: run another square array across the output matrix and at each <strong>stride</strong> keep only the max, min, or average value. In this implementation I use the max value, but any value will evenly reduce the size of the output while keeping the most important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def __init__(self, pool_size=2):\n",
    "        self.size = pool_size\n",
    "    \n",
    "    def iter_regions(self, image):\n",
    "        _, h, w = image.shape\n",
    "        new_h = h // self.size\n",
    "        new_w = w // self.size\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[:, (i * self.size):(i * self.size + self.size), (j * self.size):(j * self.size + self.size)]\n",
    "                yield im_region, i, j\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.prev_input = input\n",
    "        num_kernels, h, w = input.shape\n",
    "        output = np.zeros((num_kernels, h // self.size, w // self.size))\n",
    "        for im_region, i, j in self.iter_regions(input):\n",
    "            output[:, i, j] = np.amax(im_region, axis=(1, 2))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_L_d_out):\n",
    "        d_L_d_input = np.zeros(self.prev_input.shape)\n",
    "        for im_region, i, j in self.iter_regions(self.prev_input):\n",
    "            f, h, w = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(1, 2))\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                        if im_region[f2, i2, j2] == amax[f2]:\n",
    "                            d_L_d_input[f2, i * self.size + i2, j * self.size + j2] = d_L_d_out[f2, i, j]\n",
    "        \n",
    "        return d_L_d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMax:\n",
    "    def __init__(self, input_len, nodes):\n",
    "        self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "        self.biases = np.zeros(nodes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Forward pass\n",
    "        flat = input.flatten()\n",
    "        input_len, nodes = self.weights.shape\n",
    "        totals = np.dot(flat, self.weights) + self.biases\n",
    "        exp = np.exp(totals)\n",
    "        # Saving forward pass for backpropagation\n",
    "        self.prev_input_shape = input.shape\n",
    "        self.prev_input = flat\n",
    "        self.prev_totals = totals\n",
    "\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        for i, gradient in enumerate(d_L_d_out):\n",
    "            # Only the gradient at the correct class is nonzero\n",
    "            if gradient == 0:\n",
    "                continue \n",
    "            # e^totals\n",
    "            t_exp = np.exp(self.prev_totals)\n",
    "            S = np.sum(t_exp)\n",
    "            # Gradients at i against totals\n",
    "            d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
    "            d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "            # Gradients of totals against weights/biases/input\n",
    "            d_t_d_w = self.prev_input\n",
    "            d_t_d_b = 1\n",
    "            d_t_d_inputs = self.weights\n",
    "            # Gradients of loss against totals\n",
    "            d_L_d_t = gradient * d_out_d_t\n",
    "            d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
    "            d_L_d_b = d_L_d_t * d_t_d_b\n",
    "            d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
    "            # Update weights and biases\n",
    "            self.weights -= learn_rate * d_L_d_w\n",
    "            self.biases -= learn_rate * d_L_d_b\n",
    "\n",
    "            return d_L_d_inputs.reshape(self.prev_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: (3, 28, 28)\n",
      "conv1: (6, 28, 28)\n",
      "relu1: (6, 28, 28)\n",
      "conv2: (6, 28, 28)\n",
      "relu2: (6, 28, 28)\n",
      "pool: (6, 14, 14)\n",
      "softmax: (10,)\n",
      "softmax: (6, 14, 14)\n",
      "pool: (6, 28, 28)\n",
      "relu2: (6, 28, 28)\n",
      "conv2: (6, 28, 28)\n",
      "relu1: (6, 28, 28)\n",
      "conv1: (3, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "testconv = ConvLayer((3, 28, 28))\n",
    "test2conv = ConvLayer((6, 28, 28))\n",
    "testrelu = ReLU()\n",
    "testpool = MaxPool()\n",
    "testsoftmax = SoftMax(1176, 10)\n",
    "\n",
    "# Test forward pass\n",
    "output = (np.arange(0, 2352).reshape(3, 28, 28) / 255) - 0.5\n",
    "print(\"input:\", output.shape)\n",
    "output = testconv.forward(output)\n",
    "print(\"conv1:\", output.shape)\n",
    "output = testrelu.forward(output)\n",
    "print(\"relu1:\", output.shape)\n",
    "output = test2conv.forward(output)\n",
    "print(\"conv2:\", output.shape)\n",
    "output = testrelu.forward(output)\n",
    "print(\"relu2:\", output.shape)\n",
    "output = testpool.forward(output)\n",
    "print(\"pool:\", output.shape)\n",
    "output = testsoftmax.forward(output)\n",
    "print(\"softmax:\", output.shape)\n",
    "\n",
    "# Test backward pass\n",
    "gradient = np.zeros(10)\n",
    "gradient[5] = -1 / output[5]\n",
    "lr = 1\n",
    "gradient = testsoftmax.backprop(gradient, lr)\n",
    "print(\"softmax:\", gradient.shape)\n",
    "gradient = testpool.backprop(gradient)\n",
    "print(\"pool:\", gradient.shape)\n",
    "gradient = testrelu.backprop(gradient, lr)\n",
    "print(\"relu2:\", gradient.shape)\n",
    "gradient = test2conv.backprop(gradient, lr)\n",
    "print(\"conv2:\", gradient.shape)\n",
    "gradient = testrelu.backprop(gradient, lr)\n",
    "print(\"relu1:\", gradient.shape)\n",
    "gradient = testconv.backprop(gradient, lr)\n",
    "print(\"conv1:\", gradient.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, input_shape, kernel_size=5, num_kernels=6, padding=0):\n",
    "        # Get input dimensions\n",
    "        input_depth, input_height, input_width = input_shape\n",
    "        self.d = input_depth\n",
    "        self.h = input_height + kernel_size - 1\n",
    "        self.w = input_width + kernel_size - 1\n",
    "        self.input_shape = input_shape\n",
    "        # Initialize kernels and bias\n",
    "        self.padding = padding\n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.pad_size = kernel_size // 2\n",
    "        self.kernel_shape = (self.num_kernels, self.d, self.kernel_size, self.kernel_size)\n",
    "        self.bias_shape = (self.num_kernels, self.h - self.kernel_size + 1, self.w - self.kernel_size + 1)\n",
    "        # Dividing mimics Xavier Initialization and reduces variance\n",
    "        self.kernels = np.random.randn(*self.kernel_shape) / (self.kernel_size * self.kernel_size)\n",
    "        self.bias = np.random.randn(*self.bias_shape) / (self.h * self.w)\n",
    "    \n",
    "    def iter_regions(self, image):\n",
    "        for i in range(self.h - self.kernel_size + 1):\n",
    "            for j in range(self.w - self.kernel_size + 1):\n",
    "                im_region = image[:, i:(i + self.kernel_size), j:(j + self.kernel_size)]\n",
    "                yield im_region, i, j\n",
    "    \n",
    "    def forward(self, input):\n",
    "        padded = np.pad(input, ((0,0), (self.pad_size, self.pad_size), (self.pad_size, self.pad_size)), mode=\"constant\", constant_values=self.padding)\n",
    "        self.prev_input = padded # Save for backpropagation, padding needed\n",
    "        self.output = np.copy(self.bias)\n",
    "        for im_region, i, j in self.iter_regions(padded):\n",
    "            self.output[:, i, j] += np.sum(im_region * self.kernels, axis=(1, 2, 3))\n",
    "        return self.output\n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        # Cross correlation for kernel gradient\n",
    "        d_L_d_kernels = np.zeros(self.kernels.shape)\n",
    "        for im_region, i, j in self.iter_regions(self.prev_input):\n",
    "            for f in range(self.num_kernels):\n",
    "                d_L_d_kernels[f] += d_L_d_out[f, i, j] * im_region \n",
    "        # Full convolution for input gradient\n",
    "        d_L_d_input = np.zeros(self.input_shape)\n",
    "        padded_d_L_d_out = np.pad(d_L_d_out, ((0,0), (self.pad_size, self.pad_size), (self.pad_size, self.pad_size)), mode=\"constant\", constant_values=0)\n",
    "        conv_kernels = np.rot90(np.moveaxis(self.kernels, 0, 1), 2, axes=(2, 3))\n",
    "        for im_region2, i, j in self.iter_regions(padded_d_L_d_out):\n",
    "            for d in range(self.d):\n",
    "                d_L_d_input[d, i, j] += np.sum(im_region2 * conv_kernels[d])\n",
    "        # Adjust by learn rate\n",
    "        self.bias -= learn_rate * d_L_d_out\n",
    "        self.kernels -= learn_rate * d_L_d_kernels\n",
    "        return d_L_d_input\n",
    "    \n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.prev_input = input\n",
    "        return np.maximum(0, input)\n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        d_L_d_out[self.prev_input < 0] = 0\n",
    "        return d_L_d_out\n",
    "    \n",
    "class MaxPool:\n",
    "    def __init__(self, pool_size=2):\n",
    "        self.size = pool_size\n",
    "    \n",
    "    def iter_regions(self, image):\n",
    "        _, h, w = image.shape\n",
    "        new_h = h // self.size\n",
    "        new_w = w // self.size\n",
    "        for i in range(new_h):\n",
    "            for j in range(new_w):\n",
    "                im_region = image[:, (i * self.size):(i * self.size + self.size), (j * self.size):(j * self.size + self.size)]\n",
    "                yield im_region, i, j\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.prev_input = input\n",
    "        num_kernels, h, w = input.shape\n",
    "        output = np.zeros((num_kernels, h // self.size, w // self.size))\n",
    "        for im_region, i, j in self.iter_regions(input):\n",
    "            output[:, i, j] = np.amax(im_region, axis=(1, 2))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backprop(self, d_L_d_out):\n",
    "        d_L_d_input = np.zeros(self.prev_input.shape)\n",
    "        for im_region, i, j in self.iter_regions(self.prev_input):\n",
    "            f, h, w = im_region.shape\n",
    "            amax = np.amax(im_region, axis=(1, 2))\n",
    "            for i2 in range(h):\n",
    "                for j2 in range(w):\n",
    "                    for f2 in range(f):\n",
    "                        if im_region[f2, i2, j2] == amax[f2]:\n",
    "                            d_L_d_input[f2, i * self.size + i2, j * self.size + j2] = d_L_d_out[f2, i, j]\n",
    "        \n",
    "        return d_L_d_input\n",
    "    \n",
    "class SoftMax:\n",
    "    def __init__(self, input_len, nodes):\n",
    "        self.weights = np.random.randn(input_len, nodes) / input_len\n",
    "        self.biases = np.zeros(nodes)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # Forward pass\n",
    "        flat = input.flatten()\n",
    "        input_len, nodes = self.weights.shape\n",
    "        totals = np.dot(flat, self.weights) + self.biases\n",
    "        exp = np.exp(totals)\n",
    "        # Saving forward pass for backpropagation\n",
    "        self.prev_input_shape = input.shape\n",
    "        self.prev_input = flat\n",
    "        self.prev_totals = totals\n",
    "\n",
    "        return exp / np.sum(exp, axis=0)\n",
    "    \n",
    "    def backprop(self, d_L_d_out, learn_rate):\n",
    "        for i, gradient in enumerate(d_L_d_out):\n",
    "            # Only the gradient at the correct class is nonzero\n",
    "            if gradient == 0:\n",
    "                continue \n",
    "            # e^totals\n",
    "            t_exp = np.exp(self.prev_totals)\n",
    "            S = np.sum(t_exp)\n",
    "            # Gradients at i against totals\n",
    "            d_out_d_t = -t_exp[i] * t_exp / (S ** 2)\n",
    "            d_out_d_t[i] = t_exp[i] * (S - t_exp[i]) / (S ** 2)\n",
    "            # Gradients of totals against weights/biases/input\n",
    "            d_t_d_w = self.prev_input\n",
    "            d_t_d_b = 1\n",
    "            d_t_d_inputs = self.weights\n",
    "            # Gradients of loss against totals\n",
    "            d_L_d_t = gradient * d_out_d_t\n",
    "            d_L_d_w = d_t_d_w[np.newaxis].T @ d_L_d_t[np.newaxis]\n",
    "            d_L_d_b = d_L_d_t * d_t_d_b\n",
    "            d_L_d_inputs = d_t_d_inputs @ d_L_d_t\n",
    "            # Update weights and biases\n",
    "            self.weights -= learn_rate * d_L_d_w\n",
    "            self.biases -= learn_rate * d_L_d_b\n",
    "\n",
    "            return d_L_d_inputs.reshape(self.prev_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 ---\n",
      "[Step 100] Past 100 steps: Average Loss 2.188 | Accuracy: 26%\n",
      "[Step 200] Past 100 steps: Average Loss 1.531 | Accuracy: 49%\n",
      "[Step 300] Past 100 steps: Average Loss 1.404 | Accuracy: 54%\n",
      "[Step 400] Past 100 steps: Average Loss 1.297 | Accuracy: 56%\n",
      "[Step 500] Past 100 steps: Average Loss 0.930 | Accuracy: 72%\n",
      "[Step 600] Past 100 steps: Average Loss 1.077 | Accuracy: 59%\n",
      "[Step 700] Past 100 steps: Average Loss 0.832 | Accuracy: 75%\n",
      "[Step 800] Past 100 steps: Average Loss 0.838 | Accuracy: 66%\n",
      "[Step 900] Past 100 steps: Average Loss 0.871 | Accuracy: 66%\n",
      "[Step 1000] Past 100 steps: Average Loss 0.905 | Accuracy: 69%\n",
      "[Step 1100] Past 100 steps: Average Loss 0.883 | Accuracy: 66%\n",
      "[Step 1200] Past 100 steps: Average Loss 0.967 | Accuracy: 61%\n",
      "[Step 1300] Past 100 steps: Average Loss 0.843 | Accuracy: 69%\n",
      "[Step 1400] Past 100 steps: Average Loss 0.846 | Accuracy: 69%\n",
      "[Step 1500] Past 100 steps: Average Loss 0.842 | Accuracy: 69%\n",
      "[Step 1600] Past 100 steps: Average Loss 0.535 | Accuracy: 83%\n",
      "[Step 1700] Past 100 steps: Average Loss 0.686 | Accuracy: 76%\n",
      "[Step 1800] Past 100 steps: Average Loss 0.798 | Accuracy: 73%\n",
      "[Step 1900] Past 100 steps: Average Loss 0.753 | Accuracy: 74%\n",
      "[Step 2000] Past 100 steps: Average Loss 0.675 | Accuracy: 77%\n",
      "[Step 2100] Past 100 steps: Average Loss 0.735 | Accuracy: 69%\n",
      "[Step 2200] Past 100 steps: Average Loss 0.787 | Accuracy: 73%\n",
      "[Step 2300] Past 100 steps: Average Loss 0.609 | Accuracy: 78%\n",
      "[Step 2400] Past 100 steps: Average Loss 0.767 | Accuracy: 80%\n",
      "[Step 2500] Past 100 steps: Average Loss 0.718 | Accuracy: 74%\n",
      "[Step 2600] Past 100 steps: Average Loss 0.637 | Accuracy: 78%\n",
      "[Step 2700] Past 100 steps: Average Loss 0.710 | Accuracy: 76%\n",
      "[Step 2800] Past 100 steps: Average Loss 0.820 | Accuracy: 73%\n",
      "[Step 2900] Past 100 steps: Average Loss 0.749 | Accuracy: 74%\n",
      "[Step 3000] Past 100 steps: Average Loss 0.855 | Accuracy: 70%\n",
      "[Step 3100] Past 100 steps: Average Loss 0.699 | Accuracy: 73%\n",
      "[Step 3200] Past 100 steps: Average Loss 0.467 | Accuracy: 84%\n",
      "[Step 3300] Past 100 steps: Average Loss 0.726 | Accuracy: 72%\n",
      "[Step 3400] Past 100 steps: Average Loss 0.725 | Accuracy: 73%\n",
      "[Step 3500] Past 100 steps: Average Loss 0.458 | Accuracy: 83%\n",
      "[Step 3600] Past 100 steps: Average Loss 0.865 | Accuracy: 67%\n",
      "[Step 3700] Past 100 steps: Average Loss 0.587 | Accuracy: 80%\n",
      "[Step 3800] Past 100 steps: Average Loss 0.521 | Accuracy: 80%\n",
      "[Step 3900] Past 100 steps: Average Loss 0.785 | Accuracy: 69%\n",
      "[Step 4000] Past 100 steps: Average Loss 0.788 | Accuracy: 73%\n",
      "[Step 4100] Past 100 steps: Average Loss 0.691 | Accuracy: 74%\n",
      "[Step 4200] Past 100 steps: Average Loss 0.464 | Accuracy: 84%\n",
      "[Step 4300] Past 100 steps: Average Loss 0.564 | Accuracy: 82%\n",
      "[Step 4400] Past 100 steps: Average Loss 0.530 | Accuracy: 75%\n",
      "[Step 4500] Past 100 steps: Average Loss 0.598 | Accuracy: 82%\n",
      "[Step 4600] Past 100 steps: Average Loss 0.491 | Accuracy: 81%\n",
      "[Step 4700] Past 100 steps: Average Loss 0.356 | Accuracy: 85%\n",
      "[Step 4800] Past 100 steps: Average Loss 0.483 | Accuracy: 82%\n",
      "[Step 4900] Past 100 steps: Average Loss 0.544 | Accuracy: 76%\n",
      "[Step 5000] Past 100 steps: Average Loss 0.550 | Accuracy: 83%\n",
      "[Step 5100] Past 100 steps: Average Loss 0.692 | Accuracy: 76%\n",
      "[Step 5200] Past 100 steps: Average Loss 0.724 | Accuracy: 70%\n",
      "[Step 5300] Past 100 steps: Average Loss 0.706 | Accuracy: 71%\n",
      "[Step 5400] Past 100 steps: Average Loss 0.617 | Accuracy: 81%\n",
      "[Step 5500] Past 100 steps: Average Loss 0.738 | Accuracy: 71%\n",
      "[Step 5600] Past 100 steps: Average Loss 0.618 | Accuracy: 79%\n",
      "[Step 5700] Past 100 steps: Average Loss 0.618 | Accuracy: 79%\n",
      "[Step 5800] Past 100 steps: Average Loss 0.511 | Accuracy: 83%\n",
      "[Step 5900] Past 100 steps: Average Loss 0.472 | Accuracy: 79%\n",
      "[Step 6000] Past 100 steps: Average Loss 0.698 | Accuracy: 75%\n",
      "[Step 6100] Past 100 steps: Average Loss 0.522 | Accuracy: 77%\n",
      "[Step 6200] Past 100 steps: Average Loss 0.534 | Accuracy: 81%\n",
      "[Step 6300] Past 100 steps: Average Loss 0.590 | Accuracy: 73%\n",
      "[Step 6400] Past 100 steps: Average Loss 0.522 | Accuracy: 82%\n",
      "[Step 6500] Past 100 steps: Average Loss 0.693 | Accuracy: 73%\n",
      "[Step 6600] Past 100 steps: Average Loss 0.534 | Accuracy: 78%\n",
      "[Step 6700] Past 100 steps: Average Loss 0.608 | Accuracy: 74%\n",
      "[Step 6800] Past 100 steps: Average Loss 0.455 | Accuracy: 83%\n",
      "[Step 6900] Past 100 steps: Average Loss 0.687 | Accuracy: 71%\n",
      "[Step 7000] Past 100 steps: Average Loss 0.511 | Accuracy: 80%\n",
      "[Step 7100] Past 100 steps: Average Loss 0.505 | Accuracy: 79%\n",
      "[Step 7200] Past 100 steps: Average Loss 0.581 | Accuracy: 80%\n",
      "[Step 7300] Past 100 steps: Average Loss 0.623 | Accuracy: 79%\n",
      "[Step 7400] Past 100 steps: Average Loss 0.638 | Accuracy: 80%\n",
      "[Step 7500] Past 100 steps: Average Loss 0.444 | Accuracy: 84%\n",
      "[Step 7600] Past 100 steps: Average Loss 0.457 | Accuracy: 85%\n",
      "[Step 7700] Past 100 steps: Average Loss 0.810 | Accuracy: 74%\n",
      "[Step 7800] Past 100 steps: Average Loss 0.540 | Accuracy: 78%\n",
      "[Step 7900] Past 100 steps: Average Loss 0.505 | Accuracy: 82%\n",
      "[Step 8000] Past 100 steps: Average Loss 0.518 | Accuracy: 76%\n",
      "[Step 8100] Past 100 steps: Average Loss 0.609 | Accuracy: 76%\n",
      "[Step 8200] Past 100 steps: Average Loss 0.451 | Accuracy: 86%\n",
      "[Step 8300] Past 100 steps: Average Loss 0.524 | Accuracy: 81%\n",
      "[Step 8400] Past 100 steps: Average Loss 0.620 | Accuracy: 79%\n",
      "[Step 8500] Past 100 steps: Average Loss 0.593 | Accuracy: 78%\n",
      "[Step 8600] Past 100 steps: Average Loss 0.547 | Accuracy: 77%\n",
      "[Step 8700] Past 100 steps: Average Loss 0.532 | Accuracy: 82%\n",
      "[Step 8800] Past 100 steps: Average Loss 0.609 | Accuracy: 79%\n",
      "[Step 8900] Past 100 steps: Average Loss 0.562 | Accuracy: 82%\n",
      "[Step 9000] Past 100 steps: Average Loss 0.760 | Accuracy: 71%\n",
      "[Step 9100] Past 100 steps: Average Loss 0.512 | Accuracy: 80%\n",
      "[Step 9200] Past 100 steps: Average Loss 0.510 | Accuracy: 81%\n",
      "[Step 9300] Past 100 steps: Average Loss 0.475 | Accuracy: 80%\n",
      "[Step 9400] Past 100 steps: Average Loss 0.525 | Accuracy: 82%\n",
      "[Step 9500] Past 100 steps: Average Loss 0.607 | Accuracy: 78%\n",
      "[Step 9600] Past 100 steps: Average Loss 0.538 | Accuracy: 83%\n",
      "[Step 9700] Past 100 steps: Average Loss 0.560 | Accuracy: 78%\n",
      "[Step 9800] Past 100 steps: Average Loss 0.676 | Accuracy: 79%\n",
      "[Step 9900] Past 100 steps: Average Loss 0.618 | Accuracy: 79%\n",
      "[Step 10000] Past 100 steps: Average Loss 0.445 | Accuracy: 79%\n",
      "[Step 10100] Past 100 steps: Average Loss 0.425 | Accuracy: 84%\n",
      "[Step 10200] Past 100 steps: Average Loss 0.410 | Accuracy: 84%\n",
      "[Step 10300] Past 100 steps: Average Loss 0.537 | Accuracy: 84%\n",
      "[Step 10400] Past 100 steps: Average Loss 0.539 | Accuracy: 81%\n",
      "[Step 10500] Past 100 steps: Average Loss 0.567 | Accuracy: 83%\n",
      "[Step 10600] Past 100 steps: Average Loss 0.342 | Accuracy: 89%\n",
      "[Step 10700] Past 100 steps: Average Loss 0.472 | Accuracy: 85%\n",
      "[Step 10800] Past 100 steps: Average Loss 0.558 | Accuracy: 84%\n",
      "[Step 10900] Past 100 steps: Average Loss 0.468 | Accuracy: 85%\n",
      "[Step 11000] Past 100 steps: Average Loss 0.376 | Accuracy: 86%\n",
      "[Step 11100] Past 100 steps: Average Loss 0.473 | Accuracy: 85%\n",
      "[Step 11200] Past 100 steps: Average Loss 0.552 | Accuracy: 85%\n",
      "[Step 11300] Past 100 steps: Average Loss 0.524 | Accuracy: 80%\n",
      "[Step 11400] Past 100 steps: Average Loss 0.439 | Accuracy: 86%\n",
      "[Step 11500] Past 100 steps: Average Loss 0.555 | Accuracy: 84%\n",
      "[Step 11600] Past 100 steps: Average Loss 0.615 | Accuracy: 84%\n",
      "[Step 11700] Past 100 steps: Average Loss 0.573 | Accuracy: 76%\n",
      "[Step 11800] Past 100 steps: Average Loss 0.518 | Accuracy: 81%\n",
      "[Step 11900] Past 100 steps: Average Loss 0.816 | Accuracy: 69%\n",
      "[Step 12000] Past 100 steps: Average Loss 0.486 | Accuracy: 83%\n",
      "[Step 12100] Past 100 steps: Average Loss 0.585 | Accuracy: 80%\n",
      "[Step 12200] Past 100 steps: Average Loss 0.620 | Accuracy: 77%\n",
      "[Step 12300] Past 100 steps: Average Loss 0.504 | Accuracy: 82%\n",
      "[Step 12400] Past 100 steps: Average Loss 0.496 | Accuracy: 83%\n",
      "[Step 12500] Past 100 steps: Average Loss 0.559 | Accuracy: 79%\n",
      "[Step 12600] Past 100 steps: Average Loss 0.361 | Accuracy: 84%\n",
      "[Step 12700] Past 100 steps: Average Loss 0.420 | Accuracy: 85%\n",
      "[Step 12800] Past 100 steps: Average Loss 0.570 | Accuracy: 81%\n",
      "[Step 12900] Past 100 steps: Average Loss 0.424 | Accuracy: 82%\n",
      "[Step 13000] Past 100 steps: Average Loss 0.378 | Accuracy: 86%\n",
      "[Step 13100] Past 100 steps: Average Loss 0.349 | Accuracy: 89%\n",
      "[Step 13200] Past 100 steps: Average Loss 0.603 | Accuracy: 81%\n",
      "[Step 13300] Past 100 steps: Average Loss 0.436 | Accuracy: 85%\n",
      "[Step 13400] Past 100 steps: Average Loss 0.569 | Accuracy: 80%\n",
      "[Step 13500] Past 100 steps: Average Loss 0.503 | Accuracy: 83%\n",
      "[Step 13600] Past 100 steps: Average Loss 0.631 | Accuracy: 75%\n",
      "[Step 13700] Past 100 steps: Average Loss 0.536 | Accuracy: 80%\n",
      "[Step 13800] Past 100 steps: Average Loss 0.489 | Accuracy: 81%\n",
      "[Step 13900] Past 100 steps: Average Loss 0.499 | Accuracy: 84%\n",
      "[Step 14000] Past 100 steps: Average Loss 0.546 | Accuracy: 80%\n",
      "[Step 14100] Past 100 steps: Average Loss 0.518 | Accuracy: 80%\n",
      "[Step 14200] Past 100 steps: Average Loss 0.531 | Accuracy: 79%\n",
      "[Step 14300] Past 100 steps: Average Loss 0.509 | Accuracy: 77%\n",
      "[Step 14400] Past 100 steps: Average Loss 0.439 | Accuracy: 84%\n",
      "[Step 14500] Past 100 steps: Average Loss 0.509 | Accuracy: 81%\n",
      "[Step 14600] Past 100 steps: Average Loss 0.465 | Accuracy: 81%\n",
      "[Step 14700] Past 100 steps: Average Loss 0.464 | Accuracy: 81%\n",
      "[Step 14800] Past 100 steps: Average Loss 0.520 | Accuracy: 82%\n",
      "[Step 14900] Past 100 steps: Average Loss 0.652 | Accuracy: 78%\n",
      "[Step 15000] Past 100 steps: Average Loss 0.498 | Accuracy: 82%\n",
      "[Step 15100] Past 100 steps: Average Loss 0.484 | Accuracy: 80%\n",
      "[Step 15200] Past 100 steps: Average Loss 0.450 | Accuracy: 84%\n",
      "[Step 15300] Past 100 steps: Average Loss 0.535 | Accuracy: 77%\n",
      "[Step 15400] Past 100 steps: Average Loss 0.531 | Accuracy: 84%\n",
      "[Step 15500] Past 100 steps: Average Loss 0.559 | Accuracy: 82%\n",
      "[Step 15600] Past 100 steps: Average Loss 0.508 | Accuracy: 80%\n",
      "[Step 15700] Past 100 steps: Average Loss 0.422 | Accuracy: 88%\n",
      "[Step 15800] Past 100 steps: Average Loss 0.489 | Accuracy: 81%\n",
      "[Step 15900] Past 100 steps: Average Loss 0.302 | Accuracy: 92%\n",
      "[Step 16000] Past 100 steps: Average Loss 0.571 | Accuracy: 79%\n",
      "[Step 16100] Past 100 steps: Average Loss 0.550 | Accuracy: 80%\n",
      "[Step 16200] Past 100 steps: Average Loss 0.373 | Accuracy: 85%\n",
      "[Step 16300] Past 100 steps: Average Loss 0.438 | Accuracy: 87%\n",
      "[Step 16400] Past 100 steps: Average Loss 0.591 | Accuracy: 75%\n",
      "[Step 16500] Past 100 steps: Average Loss 0.428 | Accuracy: 89%\n",
      "[Step 16600] Past 100 steps: Average Loss 0.610 | Accuracy: 77%\n",
      "[Step 16700] Past 100 steps: Average Loss 0.508 | Accuracy: 85%\n",
      "[Step 16800] Past 100 steps: Average Loss 0.567 | Accuracy: 73%\n",
      "[Step 16900] Past 100 steps: Average Loss 0.629 | Accuracy: 76%\n",
      "[Step 17000] Past 100 steps: Average Loss 0.370 | Accuracy: 89%\n",
      "[Step 17100] Past 100 steps: Average Loss 0.518 | Accuracy: 84%\n",
      "[Step 17200] Past 100 steps: Average Loss 0.454 | Accuracy: 84%\n",
      "[Step 17300] Past 100 steps: Average Loss 0.586 | Accuracy: 80%\n",
      "[Step 17400] Past 100 steps: Average Loss 0.675 | Accuracy: 76%\n",
      "[Step 17500] Past 100 steps: Average Loss 0.506 | Accuracy: 82%\n",
      "[Step 17600] Past 100 steps: Average Loss 0.474 | Accuracy: 83%\n",
      "[Step 17700] Past 100 steps: Average Loss 0.673 | Accuracy: 79%\n",
      "[Step 17800] Past 100 steps: Average Loss 0.426 | Accuracy: 78%\n",
      "[Step 17900] Past 100 steps: Average Loss 0.490 | Accuracy: 83%\n",
      "[Step 18000] Past 100 steps: Average Loss 0.543 | Accuracy: 81%\n",
      "[Step 18100] Past 100 steps: Average Loss 0.659 | Accuracy: 75%\n",
      "[Step 18200] Past 100 steps: Average Loss 0.530 | Accuracy: 79%\n",
      "[Step 18300] Past 100 steps: Average Loss 0.571 | Accuracy: 82%\n",
      "[Step 18400] Past 100 steps: Average Loss 0.397 | Accuracy: 87%\n",
      "[Step 18500] Past 100 steps: Average Loss 0.529 | Accuracy: 77%\n",
      "[Step 18600] Past 100 steps: Average Loss 0.462 | Accuracy: 86%\n",
      "[Step 18700] Past 100 steps: Average Loss 0.393 | Accuracy: 83%\n",
      "[Step 18800] Past 100 steps: Average Loss 0.435 | Accuracy: 84%\n",
      "[Step 18900] Past 100 steps: Average Loss 0.442 | Accuracy: 85%\n",
      "[Step 19000] Past 100 steps: Average Loss 0.435 | Accuracy: 85%\n",
      "[Step 19100] Past 100 steps: Average Loss 0.721 | Accuracy: 70%\n",
      "[Step 19200] Past 100 steps: Average Loss 0.549 | Accuracy: 76%\n",
      "[Step 19300] Past 100 steps: Average Loss 0.525 | Accuracy: 85%\n",
      "[Step 19400] Past 100 steps: Average Loss 0.478 | Accuracy: 86%\n",
      "[Step 19500] Past 100 steps: Average Loss 0.539 | Accuracy: 77%\n",
      "[Step 19600] Past 100 steps: Average Loss 0.486 | Accuracy: 81%\n",
      "[Step 19700] Past 100 steps: Average Loss 0.242 | Accuracy: 90%\n",
      "[Step 19800] Past 100 steps: Average Loss 0.336 | Accuracy: 87%\n",
      "[Step 19900] Past 100 steps: Average Loss 0.549 | Accuracy: 79%\n",
      "[Step 20000] Past 100 steps: Average Loss 0.653 | Accuracy: 76%\n",
      "[Step 20100] Past 100 steps: Average Loss 0.386 | Accuracy: 84%\n",
      "[Step 20200] Past 100 steps: Average Loss 0.512 | Accuracy: 84%\n",
      "[Step 20300] Past 100 steps: Average Loss 0.470 | Accuracy: 82%\n",
      "[Step 20400] Past 100 steps: Average Loss 0.479 | Accuracy: 88%\n",
      "[Step 20500] Past 100 steps: Average Loss 0.385 | Accuracy: 86%\n",
      "[Step 20600] Past 100 steps: Average Loss 0.544 | Accuracy: 85%\n",
      "[Step 20700] Past 100 steps: Average Loss 0.585 | Accuracy: 77%\n",
      "[Step 20800] Past 100 steps: Average Loss 0.465 | Accuracy: 83%\n",
      "[Step 20900] Past 100 steps: Average Loss 0.496 | Accuracy: 80%\n",
      "[Step 21000] Past 100 steps: Average Loss 0.410 | Accuracy: 85%\n",
      "[Step 21100] Past 100 steps: Average Loss 0.480 | Accuracy: 84%\n",
      "[Step 21200] Past 100 steps: Average Loss 0.397 | Accuracy: 87%\n",
      "[Step 21300] Past 100 steps: Average Loss 0.401 | Accuracy: 82%\n",
      "[Step 21400] Past 100 steps: Average Loss 0.522 | Accuracy: 80%\n",
      "[Step 21500] Past 100 steps: Average Loss 0.465 | Accuracy: 85%\n",
      "[Step 21600] Past 100 steps: Average Loss 0.322 | Accuracy: 87%\n",
      "[Step 21700] Past 100 steps: Average Loss 0.390 | Accuracy: 85%\n",
      "[Step 21800] Past 100 steps: Average Loss 0.442 | Accuracy: 82%\n",
      "[Step 21900] Past 100 steps: Average Loss 0.566 | Accuracy: 81%\n",
      "[Step 22000] Past 100 steps: Average Loss 0.452 | Accuracy: 85%\n",
      "[Step 22100] Past 100 steps: Average Loss 0.422 | Accuracy: 87%\n",
      "[Step 22200] Past 100 steps: Average Loss 0.728 | Accuracy: 79%\n",
      "[Step 22300] Past 100 steps: Average Loss 0.547 | Accuracy: 83%\n",
      "[Step 22400] Past 100 steps: Average Loss 0.417 | Accuracy: 88%\n",
      "[Step 22500] Past 100 steps: Average Loss 0.262 | Accuracy: 90%\n",
      "[Step 22600] Past 100 steps: Average Loss 0.782 | Accuracy: 80%\n",
      "[Step 22700] Past 100 steps: Average Loss 0.403 | Accuracy: 86%\n",
      "[Step 22800] Past 100 steps: Average Loss 0.438 | Accuracy: 84%\n",
      "[Step 22900] Past 100 steps: Average Loss 0.497 | Accuracy: 83%\n",
      "[Step 23000] Past 100 steps: Average Loss 0.494 | Accuracy: 84%\n",
      "[Step 23100] Past 100 steps: Average Loss 0.582 | Accuracy: 76%\n",
      "[Step 23200] Past 100 steps: Average Loss 0.451 | Accuracy: 83%\n",
      "[Step 23300] Past 100 steps: Average Loss 0.377 | Accuracy: 85%\n",
      "[Step 23400] Past 100 steps: Average Loss 0.428 | Accuracy: 84%\n",
      "[Step 23500] Past 100 steps: Average Loss 0.501 | Accuracy: 83%\n",
      "[Step 23600] Past 100 steps: Average Loss 0.459 | Accuracy: 84%\n",
      "[Step 23700] Past 100 steps: Average Loss 0.518 | Accuracy: 82%\n",
      "[Step 23800] Past 100 steps: Average Loss 0.369 | Accuracy: 89%\n",
      "[Step 23900] Past 100 steps: Average Loss 0.323 | Accuracy: 90%\n",
      "[Step 24000] Past 100 steps: Average Loss 0.344 | Accuracy: 87%\n",
      "[Step 24100] Past 100 steps: Average Loss 0.709 | Accuracy: 77%\n",
      "[Step 24200] Past 100 steps: Average Loss 0.404 | Accuracy: 87%\n",
      "[Step 24300] Past 100 steps: Average Loss 0.410 | Accuracy: 85%\n",
      "[Step 24400] Past 100 steps: Average Loss 0.486 | Accuracy: 88%\n",
      "[Step 24500] Past 100 steps: Average Loss 0.475 | Accuracy: 86%\n",
      "[Step 24600] Past 100 steps: Average Loss 0.698 | Accuracy: 79%\n",
      "[Step 24700] Past 100 steps: Average Loss 0.452 | Accuracy: 85%\n",
      "[Step 24800] Past 100 steps: Average Loss 0.434 | Accuracy: 87%\n",
      "[Step 24900] Past 100 steps: Average Loss 0.538 | Accuracy: 82%\n",
      "[Step 25000] Past 100 steps: Average Loss 0.371 | Accuracy: 87%\n",
      "[Step 25100] Past 100 steps: Average Loss 0.304 | Accuracy: 91%\n",
      "[Step 25200] Past 100 steps: Average Loss 0.334 | Accuracy: 88%\n",
      "[Step 25300] Past 100 steps: Average Loss 0.353 | Accuracy: 89%\n",
      "[Step 25400] Past 100 steps: Average Loss 0.398 | Accuracy: 88%\n",
      "[Step 25500] Past 100 steps: Average Loss 0.434 | Accuracy: 84%\n",
      "[Step 25600] Past 100 steps: Average Loss 0.536 | Accuracy: 81%\n",
      "[Step 25700] Past 100 steps: Average Loss 0.449 | Accuracy: 83%\n",
      "[Step 25800] Past 100 steps: Average Loss 0.475 | Accuracy: 81%\n",
      "[Step 25900] Past 100 steps: Average Loss 0.326 | Accuracy: 89%\n",
      "[Step 26000] Past 100 steps: Average Loss 0.460 | Accuracy: 82%\n",
      "[Step 26100] Past 100 steps: Average Loss 0.513 | Accuracy: 80%\n",
      "[Step 26200] Past 100 steps: Average Loss 0.379 | Accuracy: 89%\n",
      "[Step 26300] Past 100 steps: Average Loss 0.718 | Accuracy: 74%\n",
      "[Step 26400] Past 100 steps: Average Loss 0.367 | Accuracy: 85%\n",
      "[Step 26500] Past 100 steps: Average Loss 0.440 | Accuracy: 86%\n",
      "[Step 26600] Past 100 steps: Average Loss 0.309 | Accuracy: 86%\n",
      "[Step 26700] Past 100 steps: Average Loss 0.344 | Accuracy: 89%\n",
      "[Step 26800] Past 100 steps: Average Loss 0.544 | Accuracy: 80%\n",
      "[Step 26900] Past 100 steps: Average Loss 0.323 | Accuracy: 87%\n",
      "[Step 27000] Past 100 steps: Average Loss 0.524 | Accuracy: 83%\n",
      "[Step 27100] Past 100 steps: Average Loss 0.248 | Accuracy: 90%\n",
      "[Step 27200] Past 100 steps: Average Loss 0.598 | Accuracy: 80%\n",
      "[Step 27300] Past 100 steps: Average Loss 0.523 | Accuracy: 81%\n",
      "[Step 27400] Past 100 steps: Average Loss 0.389 | Accuracy: 91%\n",
      "[Step 27500] Past 100 steps: Average Loss 0.398 | Accuracy: 85%\n",
      "[Step 27600] Past 100 steps: Average Loss 0.457 | Accuracy: 84%\n",
      "[Step 27700] Past 100 steps: Average Loss 0.696 | Accuracy: 72%\n",
      "[Step 27800] Past 100 steps: Average Loss 0.417 | Accuracy: 84%\n",
      "[Step 27900] Past 100 steps: Average Loss 0.342 | Accuracy: 90%\n",
      "[Step 28000] Past 100 steps: Average Loss 0.356 | Accuracy: 87%\n",
      "[Step 28100] Past 100 steps: Average Loss 0.486 | Accuracy: 86%\n",
      "[Step 28200] Past 100 steps: Average Loss 0.429 | Accuracy: 83%\n",
      "[Step 28300] Past 100 steps: Average Loss 0.545 | Accuracy: 83%\n",
      "[Step 28400] Past 100 steps: Average Loss 0.514 | Accuracy: 84%\n",
      "[Step 28500] Past 100 steps: Average Loss 0.443 | Accuracy: 85%\n",
      "[Step 28600] Past 100 steps: Average Loss 0.425 | Accuracy: 81%\n",
      "[Step 28700] Past 100 steps: Average Loss 0.436 | Accuracy: 85%\n",
      "[Step 28800] Past 100 steps: Average Loss 0.388 | Accuracy: 86%\n",
      "[Step 28900] Past 100 steps: Average Loss 0.493 | Accuracy: 81%\n",
      "[Step 29000] Past 100 steps: Average Loss 0.372 | Accuracy: 85%\n",
      "[Step 29100] Past 100 steps: Average Loss 0.607 | Accuracy: 86%\n",
      "[Step 29200] Past 100 steps: Average Loss 0.482 | Accuracy: 86%\n",
      "[Step 29300] Past 100 steps: Average Loss 0.446 | Accuracy: 86%\n",
      "[Step 29400] Past 100 steps: Average Loss 0.428 | Accuracy: 86%\n",
      "[Step 29500] Past 100 steps: Average Loss 0.571 | Accuracy: 82%\n",
      "[Step 29600] Past 100 steps: Average Loss 0.559 | Accuracy: 79%\n",
      "[Step 29700] Past 100 steps: Average Loss 0.477 | Accuracy: 83%\n",
      "[Step 29800] Past 100 steps: Average Loss 0.480 | Accuracy: 87%\n",
      "[Step 29900] Past 100 steps: Average Loss 0.457 | Accuracy: 80%\n",
      "[Step 30000] Past 100 steps: Average Loss 0.356 | Accuracy: 90%\n",
      "[Step 30100] Past 100 steps: Average Loss 0.486 | Accuracy: 83%\n",
      "[Step 30200] Past 100 steps: Average Loss 0.593 | Accuracy: 79%\n",
      "[Step 30300] Past 100 steps: Average Loss 0.418 | Accuracy: 84%\n",
      "[Step 30400] Past 100 steps: Average Loss 0.268 | Accuracy: 90%\n",
      "[Step 30500] Past 100 steps: Average Loss 0.359 | Accuracy: 87%\n",
      "[Step 30600] Past 100 steps: Average Loss 0.435 | Accuracy: 85%\n",
      "[Step 30700] Past 100 steps: Average Loss 0.455 | Accuracy: 85%\n",
      "[Step 30800] Past 100 steps: Average Loss 0.400 | Accuracy: 88%\n",
      "[Step 30900] Past 100 steps: Average Loss 0.380 | Accuracy: 89%\n",
      "[Step 31000] Past 100 steps: Average Loss 0.600 | Accuracy: 77%\n",
      "[Step 31100] Past 100 steps: Average Loss 0.290 | Accuracy: 91%\n",
      "[Step 31200] Past 100 steps: Average Loss 0.418 | Accuracy: 84%\n",
      "[Step 31300] Past 100 steps: Average Loss 0.550 | Accuracy: 83%\n",
      "[Step 31400] Past 100 steps: Average Loss 0.360 | Accuracy: 91%\n",
      "[Step 31500] Past 100 steps: Average Loss 0.402 | Accuracy: 86%\n",
      "[Step 31600] Past 100 steps: Average Loss 0.429 | Accuracy: 85%\n",
      "[Step 31700] Past 100 steps: Average Loss 0.463 | Accuracy: 84%\n",
      "[Step 31800] Past 100 steps: Average Loss 0.501 | Accuracy: 84%\n",
      "[Step 31900] Past 100 steps: Average Loss 0.281 | Accuracy: 91%\n",
      "[Step 32000] Past 100 steps: Average Loss 0.250 | Accuracy: 95%\n",
      "[Step 32100] Past 100 steps: Average Loss 0.495 | Accuracy: 80%\n",
      "[Step 32200] Past 100 steps: Average Loss 0.381 | Accuracy: 86%\n",
      "[Step 32300] Past 100 steps: Average Loss 0.455 | Accuracy: 86%\n",
      "[Step 32400] Past 100 steps: Average Loss 0.368 | Accuracy: 87%\n",
      "[Step 32500] Past 100 steps: Average Loss 0.517 | Accuracy: 84%\n",
      "[Step 32600] Past 100 steps: Average Loss 0.300 | Accuracy: 92%\n",
      "[Step 32700] Past 100 steps: Average Loss 0.344 | Accuracy: 86%\n",
      "[Step 32800] Past 100 steps: Average Loss 0.417 | Accuracy: 80%\n",
      "[Step 32900] Past 100 steps: Average Loss 0.478 | Accuracy: 83%\n",
      "[Step 33000] Past 100 steps: Average Loss 0.416 | Accuracy: 83%\n",
      "[Step 33100] Past 100 steps: Average Loss 0.450 | Accuracy: 86%\n",
      "[Step 33200] Past 100 steps: Average Loss 0.321 | Accuracy: 89%\n",
      "[Step 33300] Past 100 steps: Average Loss 0.558 | Accuracy: 81%\n",
      "[Step 33400] Past 100 steps: Average Loss 0.368 | Accuracy: 85%\n",
      "[Step 33500] Past 100 steps: Average Loss 0.370 | Accuracy: 83%\n",
      "[Step 33600] Past 100 steps: Average Loss 0.728 | Accuracy: 80%\n",
      "[Step 33700] Past 100 steps: Average Loss 0.382 | Accuracy: 86%\n",
      "[Step 33800] Past 100 steps: Average Loss 0.620 | Accuracy: 81%\n",
      "[Step 33900] Past 100 steps: Average Loss 0.434 | Accuracy: 85%\n",
      "[Step 34000] Past 100 steps: Average Loss 0.568 | Accuracy: 87%\n",
      "[Step 34100] Past 100 steps: Average Loss 0.465 | Accuracy: 83%\n",
      "[Step 34200] Past 100 steps: Average Loss 0.411 | Accuracy: 81%\n",
      "[Step 34300] Past 100 steps: Average Loss 0.371 | Accuracy: 88%\n",
      "[Step 34400] Past 100 steps: Average Loss 0.530 | Accuracy: 83%\n",
      "[Step 34500] Past 100 steps: Average Loss 0.370 | Accuracy: 85%\n",
      "[Step 34600] Past 100 steps: Average Loss 0.295 | Accuracy: 89%\n",
      "[Step 34700] Past 100 steps: Average Loss 0.399 | Accuracy: 88%\n",
      "[Step 34800] Past 100 steps: Average Loss 0.509 | Accuracy: 82%\n",
      "[Step 34900] Past 100 steps: Average Loss 0.567 | Accuracy: 81%\n",
      "[Step 35000] Past 100 steps: Average Loss 0.439 | Accuracy: 85%\n",
      "[Step 35100] Past 100 steps: Average Loss 0.265 | Accuracy: 89%\n",
      "[Step 35200] Past 100 steps: Average Loss 0.427 | Accuracy: 83%\n",
      "[Step 35300] Past 100 steps: Average Loss 0.412 | Accuracy: 84%\n",
      "[Step 35400] Past 100 steps: Average Loss 0.572 | Accuracy: 86%\n",
      "[Step 35500] Past 100 steps: Average Loss 0.367 | Accuracy: 86%\n",
      "[Step 35600] Past 100 steps: Average Loss 0.522 | Accuracy: 86%\n",
      "[Step 35700] Past 100 steps: Average Loss 0.602 | Accuracy: 85%\n",
      "[Step 35800] Past 100 steps: Average Loss 0.460 | Accuracy: 84%\n",
      "[Step 35900] Past 100 steps: Average Loss 0.381 | Accuracy: 87%\n",
      "[Step 36000] Past 100 steps: Average Loss 0.333 | Accuracy: 89%\n",
      "[Step 36100] Past 100 steps: Average Loss 0.642 | Accuracy: 81%\n",
      "[Step 36200] Past 100 steps: Average Loss 0.333 | Accuracy: 89%\n",
      "[Step 36300] Past 100 steps: Average Loss 0.452 | Accuracy: 85%\n",
      "[Step 36400] Past 100 steps: Average Loss 0.505 | Accuracy: 86%\n",
      "[Step 36500] Past 100 steps: Average Loss 0.504 | Accuracy: 79%\n",
      "[Step 36600] Past 100 steps: Average Loss 0.482 | Accuracy: 84%\n",
      "[Step 36700] Past 100 steps: Average Loss 0.514 | Accuracy: 82%\n",
      "[Step 36800] Past 100 steps: Average Loss 0.528 | Accuracy: 80%\n",
      "[Step 36900] Past 100 steps: Average Loss 0.420 | Accuracy: 85%\n",
      "[Step 37000] Past 100 steps: Average Loss 0.369 | Accuracy: 88%\n",
      "[Step 37100] Past 100 steps: Average Loss 0.404 | Accuracy: 87%\n",
      "[Step 37200] Past 100 steps: Average Loss 0.410 | Accuracy: 89%\n",
      "[Step 37300] Past 100 steps: Average Loss 0.423 | Accuracy: 88%\n",
      "[Step 37400] Past 100 steps: Average Loss 0.509 | Accuracy: 82%\n",
      "[Step 37500] Past 100 steps: Average Loss 0.371 | Accuracy: 86%\n",
      "[Step 37600] Past 100 steps: Average Loss 0.377 | Accuracy: 89%\n",
      "[Step 37700] Past 100 steps: Average Loss 0.386 | Accuracy: 86%\n",
      "[Step 37800] Past 100 steps: Average Loss 0.360 | Accuracy: 85%\n",
      "[Step 37900] Past 100 steps: Average Loss 0.348 | Accuracy: 90%\n",
      "[Step 38000] Past 100 steps: Average Loss 0.578 | Accuracy: 81%\n",
      "[Step 38100] Past 100 steps: Average Loss 0.442 | Accuracy: 90%\n",
      "[Step 38200] Past 100 steps: Average Loss 0.597 | Accuracy: 84%\n",
      "[Step 38300] Past 100 steps: Average Loss 0.407 | Accuracy: 89%\n",
      "[Step 38400] Past 100 steps: Average Loss 0.343 | Accuracy: 89%\n",
      "[Step 38500] Past 100 steps: Average Loss 0.363 | Accuracy: 88%\n",
      "[Step 38600] Past 100 steps: Average Loss 0.477 | Accuracy: 82%\n",
      "[Step 38700] Past 100 steps: Average Loss 0.451 | Accuracy: 82%\n",
      "[Step 38800] Past 100 steps: Average Loss 0.384 | Accuracy: 86%\n",
      "[Step 38900] Past 100 steps: Average Loss 0.545 | Accuracy: 81%\n",
      "[Step 39000] Past 100 steps: Average Loss 0.292 | Accuracy: 87%\n",
      "[Step 39100] Past 100 steps: Average Loss 0.349 | Accuracy: 88%\n",
      "[Step 39200] Past 100 steps: Average Loss 0.417 | Accuracy: 88%\n",
      "[Step 39300] Past 100 steps: Average Loss 0.483 | Accuracy: 80%\n",
      "[Step 39400] Past 100 steps: Average Loss 0.346 | Accuracy: 89%\n",
      "[Step 39500] Past 100 steps: Average Loss 0.409 | Accuracy: 84%\n",
      "[Step 39600] Past 100 steps: Average Loss 0.379 | Accuracy: 87%\n",
      "[Step 39700] Past 100 steps: Average Loss 0.410 | Accuracy: 89%\n",
      "[Step 39800] Past 100 steps: Average Loss 0.525 | Accuracy: 82%\n",
      "[Step 39900] Past 100 steps: Average Loss 0.353 | Accuracy: 90%\n",
      "[Step 40000] Past 100 steps: Average Loss 0.423 | Accuracy: 85%\n",
      "[Step 40100] Past 100 steps: Average Loss 0.503 | Accuracy: 81%\n",
      "[Step 40200] Past 100 steps: Average Loss 0.595 | Accuracy: 82%\n",
      "[Step 40300] Past 100 steps: Average Loss 0.256 | Accuracy: 93%\n",
      "[Step 40400] Past 100 steps: Average Loss 0.450 | Accuracy: 86%\n",
      "[Step 40500] Past 100 steps: Average Loss 0.410 | Accuracy: 83%\n",
      "[Step 40600] Past 100 steps: Average Loss 0.513 | Accuracy: 89%\n",
      "[Step 40700] Past 100 steps: Average Loss 0.313 | Accuracy: 91%\n",
      "[Step 40800] Past 100 steps: Average Loss 0.446 | Accuracy: 83%\n",
      "[Step 40900] Past 100 steps: Average Loss 0.489 | Accuracy: 84%\n",
      "[Step 41000] Past 100 steps: Average Loss 0.421 | Accuracy: 91%\n",
      "[Step 41100] Past 100 steps: Average Loss 0.440 | Accuracy: 87%\n",
      "[Step 41200] Past 100 steps: Average Loss 0.473 | Accuracy: 84%\n",
      "[Step 41300] Past 100 steps: Average Loss 0.444 | Accuracy: 85%\n",
      "[Step 41400] Past 100 steps: Average Loss 0.444 | Accuracy: 86%\n",
      "[Step 41500] Past 100 steps: Average Loss 0.333 | Accuracy: 91%\n",
      "[Step 41600] Past 100 steps: Average Loss 0.372 | Accuracy: 87%\n",
      "[Step 41700] Past 100 steps: Average Loss 0.401 | Accuracy: 85%\n",
      "[Step 41800] Past 100 steps: Average Loss 0.310 | Accuracy: 86%\n",
      "[Step 41900] Past 100 steps: Average Loss 0.342 | Accuracy: 87%\n",
      "[Step 42000] Past 100 steps: Average Loss 0.560 | Accuracy: 77%\n",
      "[Step 42100] Past 100 steps: Average Loss 0.459 | Accuracy: 85%\n",
      "[Step 42200] Past 100 steps: Average Loss 0.457 | Accuracy: 86%\n",
      "[Step 42300] Past 100 steps: Average Loss 0.341 | Accuracy: 88%\n",
      "[Step 42400] Past 100 steps: Average Loss 0.487 | Accuracy: 78%\n",
      "[Step 42500] Past 100 steps: Average Loss 0.347 | Accuracy: 86%\n",
      "[Step 42600] Past 100 steps: Average Loss 0.365 | Accuracy: 88%\n",
      "[Step 42700] Past 100 steps: Average Loss 0.395 | Accuracy: 91%\n",
      "[Step 42800] Past 100 steps: Average Loss 0.390 | Accuracy: 85%\n",
      "[Step 42900] Past 100 steps: Average Loss 0.374 | Accuracy: 86%\n",
      "[Step 43000] Past 100 steps: Average Loss 0.441 | Accuracy: 85%\n",
      "[Step 43100] Past 100 steps: Average Loss 0.362 | Accuracy: 86%\n",
      "[Step 43200] Past 100 steps: Average Loss 0.370 | Accuracy: 87%\n",
      "[Step 43300] Past 100 steps: Average Loss 0.477 | Accuracy: 89%\n",
      "[Step 43400] Past 100 steps: Average Loss 0.477 | Accuracy: 82%\n",
      "[Step 43500] Past 100 steps: Average Loss 0.351 | Accuracy: 84%\n",
      "[Step 43600] Past 100 steps: Average Loss 0.280 | Accuracy: 90%\n",
      "[Step 43700] Past 100 steps: Average Loss 0.384 | Accuracy: 87%\n",
      "[Step 43800] Past 100 steps: Average Loss 0.411 | Accuracy: 85%\n",
      "[Step 43900] Past 100 steps: Average Loss 0.482 | Accuracy: 86%\n",
      "[Step 44000] Past 100 steps: Average Loss 0.375 | Accuracy: 89%\n",
      "[Step 44100] Past 100 steps: Average Loss 0.348 | Accuracy: 87%\n",
      "[Step 44200] Past 100 steps: Average Loss 0.419 | Accuracy: 84%\n",
      "[Step 44300] Past 100 steps: Average Loss 0.574 | Accuracy: 80%\n",
      "[Step 44400] Past 100 steps: Average Loss 0.529 | Accuracy: 83%\n",
      "[Step 44500] Past 100 steps: Average Loss 0.371 | Accuracy: 88%\n",
      "[Step 44600] Past 100 steps: Average Loss 0.316 | Accuracy: 85%\n",
      "[Step 44700] Past 100 steps: Average Loss 0.358 | Accuracy: 86%\n",
      "[Step 44800] Past 100 steps: Average Loss 0.472 | Accuracy: 82%\n",
      "[Step 44900] Past 100 steps: Average Loss 0.429 | Accuracy: 85%\n",
      "[Step 45000] Past 100 steps: Average Loss 0.590 | Accuracy: 82%\n",
      "[Step 45100] Past 100 steps: Average Loss 0.471 | Accuracy: 79%\n",
      "[Step 45200] Past 100 steps: Average Loss 0.414 | Accuracy: 82%\n",
      "[Step 45300] Past 100 steps: Average Loss 0.555 | Accuracy: 81%\n",
      "[Step 45400] Past 100 steps: Average Loss 0.522 | Accuracy: 85%\n",
      "[Step 45500] Past 100 steps: Average Loss 0.385 | Accuracy: 87%\n",
      "[Step 45600] Past 100 steps: Average Loss 0.479 | Accuracy: 87%\n",
      "[Step 45700] Past 100 steps: Average Loss 0.407 | Accuracy: 88%\n",
      "[Step 45800] Past 100 steps: Average Loss 0.452 | Accuracy: 82%\n",
      "[Step 45900] Past 100 steps: Average Loss 0.499 | Accuracy: 83%\n",
      "[Step 46000] Past 100 steps: Average Loss 0.384 | Accuracy: 86%\n",
      "[Step 46100] Past 100 steps: Average Loss 0.382 | Accuracy: 87%\n",
      "[Step 46200] Past 100 steps: Average Loss 0.510 | Accuracy: 82%\n",
      "[Step 46300] Past 100 steps: Average Loss 0.403 | Accuracy: 84%\n",
      "[Step 46400] Past 100 steps: Average Loss 0.368 | Accuracy: 90%\n",
      "[Step 46500] Past 100 steps: Average Loss 0.320 | Accuracy: 85%\n",
      "[Step 46600] Past 100 steps: Average Loss 0.267 | Accuracy: 91%\n",
      "[Step 46700] Past 100 steps: Average Loss 0.470 | Accuracy: 83%\n",
      "[Step 46800] Past 100 steps: Average Loss 0.334 | Accuracy: 89%\n",
      "[Step 46900] Past 100 steps: Average Loss 0.419 | Accuracy: 83%\n",
      "[Step 47000] Past 100 steps: Average Loss 0.456 | Accuracy: 83%\n",
      "[Step 47100] Past 100 steps: Average Loss 0.361 | Accuracy: 90%\n",
      "[Step 47200] Past 100 steps: Average Loss 0.560 | Accuracy: 77%\n",
      "[Step 47300] Past 100 steps: Average Loss 0.496 | Accuracy: 79%\n",
      "[Step 47400] Past 100 steps: Average Loss 0.398 | Accuracy: 83%\n",
      "[Step 47500] Past 100 steps: Average Loss 0.352 | Accuracy: 86%\n",
      "[Step 47600] Past 100 steps: Average Loss 0.650 | Accuracy: 85%\n",
      "[Step 47700] Past 100 steps: Average Loss 0.349 | Accuracy: 91%\n",
      "[Step 47800] Past 100 steps: Average Loss 0.339 | Accuracy: 86%\n",
      "[Step 47900] Past 100 steps: Average Loss 0.234 | Accuracy: 93%\n",
      "[Step 48000] Past 100 steps: Average Loss 0.407 | Accuracy: 88%\n",
      "[Step 48100] Past 100 steps: Average Loss 0.369 | Accuracy: 85%\n",
      "[Step 48200] Past 100 steps: Average Loss 0.389 | Accuracy: 83%\n",
      "[Step 48300] Past 100 steps: Average Loss 0.454 | Accuracy: 81%\n",
      "[Step 48400] Past 100 steps: Average Loss 0.345 | Accuracy: 89%\n",
      "[Step 48500] Past 100 steps: Average Loss 0.313 | Accuracy: 89%\n",
      "[Step 48600] Past 100 steps: Average Loss 0.344 | Accuracy: 88%\n",
      "[Step 48700] Past 100 steps: Average Loss 0.529 | Accuracy: 78%\n",
      "[Step 48800] Past 100 steps: Average Loss 0.507 | Accuracy: 85%\n",
      "[Step 48900] Past 100 steps: Average Loss 0.345 | Accuracy: 88%\n",
      "[Step 49000] Past 100 steps: Average Loss 0.354 | Accuracy: 85%\n",
      "[Step 49100] Past 100 steps: Average Loss 0.457 | Accuracy: 85%\n",
      "[Step 49200] Past 100 steps: Average Loss 0.356 | Accuracy: 91%\n",
      "[Step 49300] Past 100 steps: Average Loss 0.317 | Accuracy: 86%\n",
      "[Step 49400] Past 100 steps: Average Loss 0.392 | Accuracy: 85%\n",
      "[Step 49500] Past 100 steps: Average Loss 0.435 | Accuracy: 86%\n",
      "[Step 49600] Past 100 steps: Average Loss 0.346 | Accuracy: 89%\n",
      "[Step 49700] Past 100 steps: Average Loss 0.381 | Accuracy: 88%\n",
      "[Step 49800] Past 100 steps: Average Loss 0.446 | Accuracy: 87%\n",
      "[Step 49900] Past 100 steps: Average Loss 0.448 | Accuracy: 82%\n",
      "[Step 50000] Past 100 steps: Average Loss 0.592 | Accuracy: 82%\n",
      "[Step 50100] Past 100 steps: Average Loss 0.545 | Accuracy: 80%\n",
      "[Step 50200] Past 100 steps: Average Loss 0.439 | Accuracy: 83%\n",
      "[Step 50300] Past 100 steps: Average Loss 0.435 | Accuracy: 87%\n",
      "[Step 50400] Past 100 steps: Average Loss 0.462 | Accuracy: 81%\n",
      "[Step 50500] Past 100 steps: Average Loss 0.320 | Accuracy: 88%\n",
      "[Step 50600] Past 100 steps: Average Loss 0.357 | Accuracy: 89%\n",
      "[Step 50700] Past 100 steps: Average Loss 0.442 | Accuracy: 85%\n",
      "[Step 50800] Past 100 steps: Average Loss 0.424 | Accuracy: 85%\n",
      "[Step 50900] Past 100 steps: Average Loss 0.381 | Accuracy: 82%\n",
      "[Step 51000] Past 100 steps: Average Loss 0.317 | Accuracy: 92%\n",
      "[Step 51100] Past 100 steps: Average Loss 0.265 | Accuracy: 92%\n",
      "[Step 51200] Past 100 steps: Average Loss 0.452 | Accuracy: 86%\n",
      "[Step 51300] Past 100 steps: Average Loss 0.544 | Accuracy: 81%\n",
      "[Step 51400] Past 100 steps: Average Loss 0.336 | Accuracy: 90%\n",
      "[Step 51500] Past 100 steps: Average Loss 0.512 | Accuracy: 84%\n",
      "[Step 51600] Past 100 steps: Average Loss 0.307 | Accuracy: 84%\n",
      "[Step 51700] Past 100 steps: Average Loss 0.457 | Accuracy: 81%\n",
      "[Step 51800] Past 100 steps: Average Loss 0.416 | Accuracy: 87%\n",
      "[Step 51900] Past 100 steps: Average Loss 0.337 | Accuracy: 90%\n",
      "[Step 52000] Past 100 steps: Average Loss 0.421 | Accuracy: 87%\n",
      "[Step 52100] Past 100 steps: Average Loss 0.289 | Accuracy: 88%\n",
      "[Step 52200] Past 100 steps: Average Loss 0.377 | Accuracy: 85%\n",
      "[Step 52300] Past 100 steps: Average Loss 0.442 | Accuracy: 83%\n",
      "[Step 52400] Past 100 steps: Average Loss 0.573 | Accuracy: 83%\n",
      "[Step 52500] Past 100 steps: Average Loss 0.409 | Accuracy: 85%\n",
      "[Step 52600] Past 100 steps: Average Loss 0.519 | Accuracy: 86%\n",
      "[Step 52700] Past 100 steps: Average Loss 0.457 | Accuracy: 84%\n",
      "[Step 52800] Past 100 steps: Average Loss 0.541 | Accuracy: 86%\n",
      "[Step 52900] Past 100 steps: Average Loss 0.322 | Accuracy: 89%\n",
      "[Step 53000] Past 100 steps: Average Loss 0.428 | Accuracy: 87%\n",
      "[Step 53100] Past 100 steps: Average Loss 0.326 | Accuracy: 87%\n",
      "[Step 53200] Past 100 steps: Average Loss 0.488 | Accuracy: 84%\n",
      "[Step 53300] Past 100 steps: Average Loss 0.448 | Accuracy: 80%\n",
      "[Step 53400] Past 100 steps: Average Loss 0.373 | Accuracy: 84%\n",
      "[Step 53500] Past 100 steps: Average Loss 0.426 | Accuracy: 85%\n",
      "[Step 53600] Past 100 steps: Average Loss 0.505 | Accuracy: 81%\n",
      "[Step 53700] Past 100 steps: Average Loss 0.462 | Accuracy: 78%\n",
      "[Step 53800] Past 100 steps: Average Loss 0.317 | Accuracy: 87%\n",
      "[Step 53900] Past 100 steps: Average Loss 0.508 | Accuracy: 83%\n",
      "[Step 54000] Past 100 steps: Average Loss 0.475 | Accuracy: 84%\n",
      "[Step 54100] Past 100 steps: Average Loss 0.388 | Accuracy: 84%\n",
      "[Step 54200] Past 100 steps: Average Loss 0.576 | Accuracy: 79%\n",
      "[Step 54300] Past 100 steps: Average Loss 0.487 | Accuracy: 81%\n",
      "[Step 54400] Past 100 steps: Average Loss 0.464 | Accuracy: 85%\n",
      "[Step 54500] Past 100 steps: Average Loss 0.467 | Accuracy: 83%\n",
      "[Step 54600] Past 100 steps: Average Loss 0.568 | Accuracy: 78%\n",
      "[Step 54700] Past 100 steps: Average Loss 0.364 | Accuracy: 85%\n",
      "[Step 54800] Past 100 steps: Average Loss 0.350 | Accuracy: 88%\n",
      "[Step 54900] Past 100 steps: Average Loss 0.538 | Accuracy: 87%\n",
      "[Step 55000] Past 100 steps: Average Loss 0.475 | Accuracy: 85%\n",
      "[Step 55100] Past 100 steps: Average Loss 0.427 | Accuracy: 82%\n",
      "[Step 55200] Past 100 steps: Average Loss 0.230 | Accuracy: 91%\n",
      "[Step 55300] Past 100 steps: Average Loss 0.361 | Accuracy: 85%\n",
      "[Step 55400] Past 100 steps: Average Loss 0.482 | Accuracy: 82%\n",
      "[Step 55500] Past 100 steps: Average Loss 0.455 | Accuracy: 84%\n",
      "[Step 55600] Past 100 steps: Average Loss 0.381 | Accuracy: 85%\n",
      "[Step 55700] Past 100 steps: Average Loss 0.339 | Accuracy: 88%\n",
      "[Step 55800] Past 100 steps: Average Loss 0.385 | Accuracy: 87%\n",
      "[Step 55900] Past 100 steps: Average Loss 0.336 | Accuracy: 88%\n",
      "[Step 56000] Past 100 steps: Average Loss 0.412 | Accuracy: 86%\n",
      "[Step 56100] Past 100 steps: Average Loss 0.444 | Accuracy: 82%\n",
      "[Step 56200] Past 100 steps: Average Loss 0.482 | Accuracy: 84%\n",
      "[Step 56300] Past 100 steps: Average Loss 0.377 | Accuracy: 84%\n",
      "[Step 56400] Past 100 steps: Average Loss 0.426 | Accuracy: 83%\n",
      "[Step 56500] Past 100 steps: Average Loss 0.465 | Accuracy: 82%\n",
      "[Step 56600] Past 100 steps: Average Loss 0.558 | Accuracy: 82%\n",
      "[Step 56700] Past 100 steps: Average Loss 0.337 | Accuracy: 87%\n",
      "[Step 56800] Past 100 steps: Average Loss 0.380 | Accuracy: 85%\n",
      "[Step 56900] Past 100 steps: Average Loss 0.447 | Accuracy: 85%\n",
      "[Step 57000] Past 100 steps: Average Loss 0.476 | Accuracy: 85%\n",
      "[Step 57100] Past 100 steps: Average Loss 0.459 | Accuracy: 81%\n",
      "[Step 57200] Past 100 steps: Average Loss 0.536 | Accuracy: 83%\n",
      "[Step 57300] Past 100 steps: Average Loss 0.426 | Accuracy: 88%\n",
      "[Step 57400] Past 100 steps: Average Loss 0.318 | Accuracy: 86%\n",
      "[Step 57500] Past 100 steps: Average Loss 0.311 | Accuracy: 87%\n",
      "[Step 57600] Past 100 steps: Average Loss 0.336 | Accuracy: 91%\n",
      "[Step 57700] Past 100 steps: Average Loss 0.373 | Accuracy: 86%\n",
      "[Step 57800] Past 100 steps: Average Loss 0.360 | Accuracy: 85%\n",
      "[Step 57900] Past 100 steps: Average Loss 0.329 | Accuracy: 86%\n",
      "[Step 58000] Past 100 steps: Average Loss 0.351 | Accuracy: 88%\n",
      "[Step 58100] Past 100 steps: Average Loss 0.460 | Accuracy: 84%\n",
      "[Step 58200] Past 100 steps: Average Loss 0.485 | Accuracy: 85%\n",
      "[Step 58300] Past 100 steps: Average Loss 0.297 | Accuracy: 88%\n",
      "[Step 58400] Past 100 steps: Average Loss 0.424 | Accuracy: 81%\n",
      "[Step 58500] Past 100 steps: Average Loss 0.528 | Accuracy: 80%\n",
      "[Step 58600] Past 100 steps: Average Loss 0.299 | Accuracy: 90%\n",
      "[Step 58700] Past 100 steps: Average Loss 0.273 | Accuracy: 90%\n",
      "[Step 58800] Past 100 steps: Average Loss 0.490 | Accuracy: 81%\n",
      "[Step 58900] Past 100 steps: Average Loss 0.344 | Accuracy: 84%\n",
      "[Step 59000] Past 100 steps: Average Loss 0.249 | Accuracy: 89%\n",
      "[Step 59100] Past 100 steps: Average Loss 0.354 | Accuracy: 86%\n",
      "[Step 59200] Past 100 steps: Average Loss 0.467 | Accuracy: 83%\n",
      "[Step 59300] Past 100 steps: Average Loss 0.419 | Accuracy: 88%\n",
      "[Step 59400] Past 100 steps: Average Loss 0.316 | Accuracy: 91%\n",
      "[Step 59500] Past 100 steps: Average Loss 0.336 | Accuracy: 85%\n",
      "[Step 59600] Past 100 steps: Average Loss 0.411 | Accuracy: 89%\n",
      "[Step 59700] Past 100 steps: Average Loss 0.495 | Accuracy: 84%\n",
      "[Step 59800] Past 100 steps: Average Loss 0.385 | Accuracy: 87%\n",
      "[Step 59900] Past 100 steps: Average Loss 0.258 | Accuracy: 90%\n",
      "[Step 60000] Past 100 steps: Average Loss 0.341 | Accuracy: 91%\n",
      "\n",
      "--- Testing the CNN ---\n",
      "Test Loss: 0.4259412568906793\n",
      "Test Accuracy: 0.8549\n"
     ]
    }
   ],
   "source": [
    "conv = ConvLayer(8)\n",
    "pool = MaxPool()\n",
    "softmax = SoftMax(12 * 12 * 8, 10)\n",
    "\n",
    "def forward(image, label):\n",
    "  '''\n",
    "  Completes a forward pass of the CNN and calculates the accuracy and\n",
    "  cross-entropy loss.\n",
    "  - image is a 2d numpy array\n",
    "  - label is a digit\n",
    "  '''\n",
    "  # We transform the image from [0, 255] to [-0.5, 0.5] to make it easier\n",
    "  # to work with. This is standard practice.\n",
    "  out = conv.forward((image / 255) - 0.5)\n",
    "  out = pool.forward(out)\n",
    "  out = softmax.forward(out)\n",
    "\n",
    "  # Calculate cross-entropy loss and accuracy. np.log() is the natural log.\n",
    "  loss = -np.log(out[label])\n",
    "  acc = 1 if np.argmax(out) == label else 0\n",
    "\n",
    "  return out, loss, acc\n",
    "\n",
    "def train(im, label, lr=.005):\n",
    "  '''\n",
    "  Completes a full training step on the given image and label.\n",
    "  Returns the cross-entropy loss and accuracy.\n",
    "  - image is a 2d numpy array\n",
    "  - label is a digit\n",
    "  - lr is the learning rate\n",
    "  '''\n",
    "  # Forward\n",
    "  out, loss, acc = forward(im, label)\n",
    "\n",
    "  # Calculate initial gradient\n",
    "  gradient = np.zeros(10)\n",
    "  gradient[label] = -1 / out[label]\n",
    "\n",
    "  # Backprop\n",
    "  gradient = softmax.backprop(gradient, lr)\n",
    "  gradient = pool.backprop(gradient)\n",
    "  gradient = conv.backprop(gradient, lr)\n",
    "\n",
    "  return loss, acc\n",
    "\n",
    "# Train the CNN for 3 epochs\n",
    "for epoch in range(1):\n",
    "  print('--- Epoch %d ---' % (epoch + 1))\n",
    "\n",
    "  # Shuffle the training data\n",
    "  permutation = np.random.permutation(len(trainset))\n",
    "  train_images = trainset[permutation]\n",
    "  train_labels = trainlabels[permutation]\n",
    "\n",
    "  # Train!\n",
    "  loss = 0\n",
    "  num_correct = 0\n",
    "  for i, (im, label) in enumerate(zip(trainset, trainlabels)):\n",
    "    if i > 0 and i % 100 == 99:\n",
    "      print(\n",
    "        '[Step %d] Past 100 steps: Average Loss %.3f | Accuracy: %d%%' %\n",
    "        (i + 1, loss / 100, num_correct)\n",
    "      )\n",
    "      loss = 0\n",
    "      num_correct = 0\n",
    "\n",
    "    l, acc = train(im, label)\n",
    "    loss += l\n",
    "    num_correct += acc\n",
    "\n",
    "# Test the CNN\n",
    "print('\\n--- Testing the CNN ---')\n",
    "loss = 0\n",
    "num_correct = 0\n",
    "for im, label in zip(testset, testlabels):\n",
    "  _, l, acc = forward(im, label)\n",
    "  loss += l\n",
    "  num_correct += acc\n",
    "\n",
    "num_tests = len(testset)\n",
    "print('Test Loss:', loss / num_tests)\n",
    "print('Test Accuracy:', num_correct / num_tests)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
